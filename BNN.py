# -*- coding: utf-8 -*-
"""
è´å¶æ–¯ç¥ç»ç½‘ç»œç»æµå˜é‡å»ºæ¨¡ - ä¸‰å±‚ç½‘ç»œç»“æ„
åŒ…å«ç½‘ç»œç»“æ„è°ƒæ•´ã€å…ƒæ•°æ®ç»“æ„ä¿®å¤å’Œå®Œæ•´é€†æ ‡å‡†åŒ–
"""
import numpy as np
import pandas as pd
import torch
import pyro
import pyro.distributions as dist
from pyro.infer import NUTS, MCMC
from pyro.infer.predictive import Predictive
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
import time
import joblib
import pickle
import os
from collections import namedtuple

# å…¨å±€é…ç½®
pyro.set_rng_seed(42)
torch.set_default_dtype(torch.float32)
torch.manual_seed(42)
np.random.seed(42)

# ä¿®æ”¹ä¸º8ä¸ªç‰¹å¾ï¼Œæ ¹æ®ä½ çš„è¦æ±‚
NUM_FEATURES = 8
HIDDEN_DIM = 5

class BayesianEconomicModel:
    def __init__(self, num_features=NUM_FEATURES, hidden_dim=HIDDEN_DIM):
        self.num_features = num_features
        self.hidden_dim = hidden_dim
        
    def model(self, X, y=None):
        # ç±»å‹æ£€æŸ¥ä¸è½¬æ¢
        X = X if isinstance(X, torch.Tensor) else torch.as_tensor(X, dtype=torch.float32)
        if y is not None and not isinstance(y, torch.Tensor):
            y = torch.as_tensor(y, dtype=torch.float32).unsqueeze(1)
            
        # å®šä¹‰ä¸‰å±‚ç½‘ç»œå‚æ•°
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚â†’éšè—å±‚1
        w1 = pyro.sample("w1", dist.Normal(0, 1.0).expand([self.hidden_dim, self.num_features]).to_event(2))
        b1 = pyro.sample("b1", dist.Normal(0, 0.5).expand([self.hidden_dim]).to_event(1))
        
        # ç¬¬äºŒå±‚ï¼šéšè—å±‚1â†’éšè—å±‚2
        w2 = pyro.sample("w2", dist.Normal(0, 1.0).expand([self.hidden_dim, self.hidden_dim]).to_event(2))
        b2 = pyro.sample("b2", dist.Normal(0, 0.5).expand([self.hidden_dim]).to_event(1))
        
        # ç¬¬ä¸‰å±‚ï¼šéšè—å±‚2â†’è¾“å‡ºå±‚
        w3 = pyro.sample("w3", dist.Normal(0, 1.0).expand([1, self.hidden_dim]).to_event(2))
        b3 = pyro.sample("b3", dist.Normal(0, 0.5).expand([1]).to_event(1))
        
        sigma = pyro.sample("sigma", dist.HalfNormal(0.5))

        # ç½‘ç»œå‰å‘ä¼ æ’­
        with pyro.plate("data", X.shape[0]):
            # ç¬¬ä¸€å±‚ + sigmoidæ¿€æ´»
            hidden1 = torch.sigmoid(torch.mm(X, w1.t()) + b1)  # ç¡®ä¿ä¸ä¸ºè´Ÿ
            # ç¬¬äºŒå±‚ + sigmoidæ¿€æ´»
            hidden2 = torch.sigmoid(torch.mm(hidden1, w2.t()) + b2)  # ç¡®ä¿ä¸ä¸ºè´Ÿ
             # ç¬¬ä¸‰å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
            mu = torch.mm(hidden2, w3.t()) + b3
    
            if y is not None:
                pyro.sample("obs", dist.Normal(mu, sigma).to_event(1), obs=y)
            else:
                pyro.sample("obs", dist.Normal(mu, sigma).to_event(1))

    def train(self, X, y, num_samples=1000, warmup=300):
        X_tensor = torch.as_tensor(X, dtype=torch.float32)
        y_tensor = torch.as_tensor(y, dtype=torch.float32).unsqueeze(1)
        
        nuts_kernel = NUTS(
            self.model,
            jit_compile=True,
            target_accept_prob=0.8,
            max_tree_depth=8
        )
        
        mcmc = MCMC(
            nuts_kernel,
            num_samples=num_samples,
            warmup_steps=warmup,
            num_chains=1,
            disable_progbar=False  # æ˜¾ç¤ºé‡‡æ ·è¿›åº¦æ¡
        )
        mcmc.run(X_tensor, y_tensor)
        
        self.samples = {
            k: v.detach().to(dtype=torch.float32) 
            for k, v in mcmc.get_samples().items()
        }
        return self
    
    def predict(self, X, num_samples=1000):
        X_tensor = torch.as_tensor(X, dtype=torch.float32)
        
        predictive = Predictive(
            self.model,
            posterior_samples=self.samples,
            return_sites=["obs"]
        )
        
        pred_samples = predictive(X_tensor)["obs"].detach().numpy()
        return pred_samples

def describe_parameter_distributions(samples):
    """è¯¦ç»†æè¿°æ‰€æœ‰å‚æ•°åˆ†å¸ƒï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰"""
    print("\n" + "="*50)
    print("å‚æ•°åˆ†å¸ƒç»Ÿè®¡é‡")
    print("="*50)
    
    # æ’é™¤è§‚æµ‹å€¼
    param_names = [k for k in samples.keys() if k != "obs"]
    
    for i, param_name in enumerate(param_names):
        param_data = samples[param_name]
        
        # å¤„ç†é«˜ç»´å‚æ•°
        if len(param_data.shape) > 1:
            # å±•å¹³é«˜ç»´å‚æ•°
            param_data = param_data.reshape(-1)
            
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_val = param_data.mean()
        std_val = param_data.std()
        
        print(f"\nâœ å‚æ•°: {param_name}")
        print(f"  å‡å€¼: {mean_val:.6f}")
        print(f"  æ ‡å‡†å·®: {std_val:.6f}")
        print(f"  å½¢çŠ¶: {samples[param_name].shape}")
        
        if len(samples[param_name].shape) > 1:
            # å¯¹äºçŸ©é˜µå‚æ•°ï¼Œæ˜¾ç¤ºæ¯åˆ—çš„å¹³å‡å€¼
            mat_mean = samples[param_name].mean(axis=0)
            print(f"  æ¯åˆ—å‡å€¼: {mat_mean}")
    
    print("="*50 + "\n")
    return True

def plot_parameter_distributions(samples, param_names=None):
    """ç»˜åˆ¶æ‰€æœ‰å‚æ•°çš„åˆ†å¸ƒ"""
    if param_names is None:
        param_names = [k for k in samples.keys() if k != "obs"]
    
    columns = 4
    rows = int(np.ceil(len(param_names) / columns))
    
    plt.figure(figsize=(15, 4 * rows))
    for i, param_name in enumerate(param_names):
        plt.subplot(rows, columns, i+1)
        param_data = samples[param_name]
        
        # å¤„ç†é«˜ç»´å‚æ•°
        if len(param_data.shape) > 1:
            # å±•å¹³é«˜ç»´å‚æ•°
            param_data = param_data.reshape(-1)
            display_name = param_name + " (flattened)"
        else:
            display_name = param_name
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_val = param_data.mean()
        std_val = param_data.std()
        
        sns.histplot(x=param_data, kde=True)
        plt.title(f"{display_name}\nå‡å€¼: {mean_val:.4f}, æ ‡å‡†å·®: {std_val:.4f}")
        plt.xlabel("å‚æ•°å€¼")
    
    plt.tight_layout()
    plt.show()
    return True

def plot_prediction_intervals(pred_samples, y_true=None, title="Predictions", xlabel="Time Step", ylabel="Value"):
    """ç»˜åˆ¶é¢„æµ‹åŒºé—´"""
    mean_pred = pred_samples.mean(0).flatten()  # ç¡®ä¿ä¸€ç»´
    lower_bound = np.percentile(pred_samples, 2.5, axis=0).flatten()  # ç¡®ä¿ä¸€ç»´
    upper_bound = np.percentile(pred_samples, 97.5, axis=0).flatten()  # ç¡®ä¿ä¸€ç»´
    
    plt.figure(figsize=(10, 5))
    if y_true is not None:
        plt.plot(y_true.flatten(), label="å®é™…å€¼", color="blue", alpha=0.7)  # å±•å¹³ä¸ºä¸€ç»´
    
    plt.plot(mean_pred, label="é¢„æµ‹å€¼", color="red")
    
    # å¡«å……95%ç½®ä¿¡åŒºé—´
    plt.fill_between(
        range(len(mean_pred)), 
        lower_bound, 
        upper_bound, 
        color="gray", alpha=0.3, label="95% ç½®ä¿¡åŒºé—´"
    )
    
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(f"{title} ä¸95%ç½®ä¿¡åŒºé—´")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{title}_prediction_intervals.png")
    plt.show()
    print(f"âœ… é¢„æµ‹åŒºé—´å›¾å·²ä¿å­˜ä¸º: {title}_prediction_intervals.png")
    return True

def inverse_transform_target(meta_data, predictions, target_idx):
    """
    å°†ç›®æ ‡å˜é‡é¢„æµ‹å€¼å®Œæ•´é€†è½¬æ¢åˆ°åŸå§‹å°ºåº¦
    æŒ‰é¡ºåºåŒ¹é…: å‡è®¾meta_dataä¸­çš„é¡ºåºä¸é¢„æµ‹é¡ºåºä¸€è‡´
    
    æ­¥éª¤:
    1. è·å–ç›®æ ‡å˜é‡çš„å…ƒæ•°æ®ï¼ˆæŒ‰é¡ºåºï¼‰
    2. é€†æ ‡å‡†åŒ–ï¼šåŸå§‹å·®åˆ†å€¼ = æ ‡å‡†åŒ–å€¼ * scale + mean
    3. é€†å·®åˆ†ï¼šç´¯åŠ å·®åˆ†å€¼å¾—åˆ°æ°´å¹³å€¼ (ä»åˆå§‹å€¼å¼€å§‹)
    
    å‚æ•°:
    predictions: å½¢çŠ¶ä¸º (n_samples, n_timesteps) çš„é¢„æµ‹å€¼
    target_idx: ç›®æ ‡å˜é‡çš„ç´¢å¼•ï¼ˆ0,1æˆ–2ï¼‰
    
    è¿”å›:
    é€†è½¬æ¢åçš„åŸå§‹å°ºåº¦å€¼ï¼Œå½¢çŠ¶ä¸º (n_samples, n_timesteps)
    """
    # 1. é€šè¿‡ç´¢å¼•è·å–ç›®æ ‡å˜é‡åç§°
    target_name = meta_data['target_names'][target_idx]
    
    # 2. è·å–å…ƒæ•°æ®
    initial_value = meta_data['initial_values'][target_name]
    scaler_params = meta_data['scaler_params'][target_name]
    
    # ç¡®ä¿æœ‰æ­£ç¡®çš„å‚æ•°
    mean = scaler_params['mean']
    scale = scaler_params['scale']
    
    # 3. é€†æ ‡å‡†åŒ–ï¼šæ¢å¤å·®åˆ†å€¼
    inv_std_preds = predictions * scale + mean
    
    # 4. é€†å·®åˆ†ï¼šä»åˆå§‹å€¼å¼€å§‹ç´¯åŠ å·®åˆ†å€¼
    raw_preds = np.zeros_like(inv_std_preds)
    
    # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé€†å·®åˆ†
    for i in range(inv_std_preds.shape[0]):
        cumulative = initial_value
        for j in range(inv_std_preds.shape[1]):
            cumulative += inv_std_preds[i, j]
            raw_preds[i, j] = cumulative
    
    return raw_preds

# =============== ä¸¥æ ¼çš„å…ƒæ•°æ®åŠ è½½å‡½æ•° ===============
def critical_load_metadata(path):
    """ä¸¥æ ¼åŠ è½½å…ƒæ•°æ®ï¼Œå‡ºé”™æ—¶ç›´æ¥æŠ¥é”™"""
    try:
        print(f"\nâ©¶â©¶â©¶â©¶â©¶â©¶ å¼€å§‹åŠ è½½å…ƒæ•°æ®: {path} â©¶â©¶â©¶â©¶â©¶â©¶")
        with open(path, 'rb') as f:
            meta_data = pickle.load(f)
        
        print("âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"å…ƒæ•°æ®ç±»å‹: {type(meta_data)}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_keys = ['target_names', 'initial_values', 'scaler_params']
        for key in required_keys:
            if key not in meta_data:
                raise ValueError(f"å…ƒæ•°æ®ç¼ºå°‘å¿…éœ€å­—æ®µ: {key}")
        print("âœ… å¿…éœ€å­—æ®µéªŒè¯é€šè¿‡")
        
        # ç¡®ä¿æ‰€æœ‰ç›®æ ‡éƒ½æœ‰åˆå§‹å€¼å’Œæ ‡å‡†åŒ–å‚æ•°
        all_targets = set(meta_data['target_names'])
        
        # éªŒè¯åˆå§‹å€¼
        if not isinstance(meta_data['initial_values'], dict):
            raise TypeError(f"initial_valuesåº”ä¸ºå­—å…¸, å®é™…ä¸º {type(meta_data['initial_values'])}")
        
        # éªŒè¯scaler_params
        if not isinstance(meta_data['scaler_params'], dict):
            raise TypeError(f"scaler_paramsåº”ä¸ºå­—å…¸, å®é™…ä¸º {type(meta_data['scaler_params'])}")
        
        # éªŒè¯æ‰€æœ‰ç›®æ ‡æ˜¯å¦éƒ½æœ‰åˆå§‹å€¼
        missing_init = all_targets - set(meta_data['initial_values'].keys())
        if missing_init:
            raise ValueError(f"ç›®æ ‡ç¼ºå¤±åˆå§‹å€¼: {', '.join(missing_init)}")
        
        # éªŒè¯æ‰€æœ‰ç›®æ ‡æ˜¯å¦éƒ½æœ‰scaler_params
        missing_scaler = all_targets - set(meta_data['scaler_params'].keys())
        if missing_scaler:
            raise ValueError(f"ç›®æ ‡ç¼ºå¤±æ ‡å‡†åŒ–å‚æ•°: {', '.join(missing_scaler)}")
        
        print("âœ… åˆå§‹å€¼å’Œæ ‡å‡†åŒ–å‚æ•°å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        
        # ç¡®ä¿åŸå§‹å€¼å’Œæ ‡å‡†åŒ–å‚æ•°éƒ½æ˜¯æ ‡é‡
        for target_name in all_targets:
            # ç¡®ä¿åˆå§‹å€¼æ˜¯æ ‡é‡
            init_val = meta_data['initial_values'][target_name]
            if isinstance(init_val, (np.ndarray, list, tuple)):
                if len(init_val) == 1:
                    meta_data['initial_values'][target_name] = float(init_val[0])
                else:
                    raise ValueError(f"åˆå§‹å€¼åº”è¯¥æ˜¯æ ‡é‡: {target_name} çš„å½¢çŠ¶æ˜¯ {np.array(init_val).shape}")
            
            # ç¡®ä¿æ ‡å‡†åŒ–å‚æ•°éƒ½æ˜¯æ ‡é‡
            scaler_params = meta_data['scaler_params'][target_name]
            for param in ['mean', 'scale']:
                param_val = scaler_params.get(param)
                if param_val is None:
                    raise ValueError(f"ç¼ºå°‘å¿…éœ€å‚æ•° '{param}' å¯¹äºç›®æ ‡ {target_name}")
                if isinstance(param_val, (np.ndarray, list, tuple)):
                    if len(param_val) == 1:
                        scaler_params[param] = float(param_val[0])
                    else:
                        raise ValueError(f"æ ‡å‡†åŒ–å‚æ•° '{param}' åº”è¯¥æ˜¯æ ‡é‡: {target_name} çš„å½¢çŠ¶æ˜¯ {np.array(param_val).shape}")
        
        # æ‰“å°å…ƒæ•°æ®è¡¨æ ¼
        print("\n" + "="*70)
        print("| {:12} | {:>14} | {:>14} | {:>14} | {:>14} |".format(
            "ç›®æ ‡å˜é‡", "åˆå§‹å€¼", "å‡å€¼(mean)", "ç¼©æ”¾ç³»æ•°(scale)", "æ ‡å‡†åŒ–åˆå§‹å€¼"))
        print("="*70)
        for target_name in meta_data['target_names']:
            mean_val = meta_data['scaler_params'][target_name]['mean']
            scale_val = meta_data['scaler_params'][target_name]['scale']
            normalized = (meta_data['initial_values'][target_name] - mean_val) / scale_val
            
            print("| {:12} | {:14.4f} | {:14.6f} | {:14.6f} | {:14.6f} |".format(
                target_name, meta_data['initial_values'][target_name], mean_val, 
                scale_val, normalized))
        print("="*70)
        
        return meta_data
    
    except Exception as e:
        print(f"\nâŒ ä¸¥é‡: å…ƒæ•°æ®åŠ è½½å¤±è´¥ âŒ\n{'-'*50}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        print("-"*50)
        print("éœ€è¦ç¡®ä¿å…ƒæ•°æ®æ–‡ä»¶åŒ…å«:")
        print("- 'target_names': ç›®æ ‡å˜é‡åç§°åˆ—è¡¨")
        print("- 'initial_values': æ¯ä¸ªç›®æ ‡çš„åˆå§‹å€¼å­—å…¸")
        print("- 'scaler_params': æ¯ä¸ªç›®æ ‡çš„æ ‡å‡†åŒ–å‚æ•°å­—å…¸ (åŒ…å«'mean'å’Œ'scale')")
        print("å…ƒæ•°æ®ç»“æ„åº”ç±»ä¼¼:")
        print({
            'target_names': ['TARGET1', 'TARGET2'],
            'initial_values': {'TARGET1': 100.0, 'TARGET2': 50.0},
            'scaler_params': {
                'TARGET1': {'mean': 90.0, 'scale': 10.0},
                'TARGET2': {'mean': 40.0, 'scale': 5.0}
            }
        })
        print("="*70)
        raise RuntimeError("å…ƒæ•°æ®åŠ è½½å¤±è´¥å¯¼è‡´ç¨‹åºç»ˆæ­¢") from e


if __name__ == "__main__":
    # 1. åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
    print(f"âœ åŠ è½½æ•°æ®ï¼Œé¢„æœŸç‰¹å¾æ•°é‡: {NUM_FEATURES}")
    pca_df = pd.read_csv(r"/mnt/pca_components.csv", parse_dates=['DATE'])
    
    # å…³é”®ä¿®æ”¹ï¼šåªè¯»å–å‰8ä¸ªæ•°å€¼åˆ—ï¼ˆè·³è¿‡ç¬¬ä¸€åˆ—DATEï¼‰
    print("âœ ç›´æ¥é€‰æ‹©å‰8ä¸ªæ•°å€¼ç‰¹å¾ï¼ˆè·³è¿‡æ—¥æœŸåˆ—ï¼‰")
    
    # è·å–æ‰€æœ‰æ•°å€¼åˆ—ï¼ˆè·³è¿‡ç¬¬ä¸€åˆ—DATEï¼‰
    numeric_cols = [col for col in pca_df.columns if col != 'DATE'][:NUM_FEATURES]
    print(f"ä½¿ç”¨çš„å‰{NUM_FEATURES}ä¸ªç‰¹å¾: {numeric_cols}")
    
    X = pca_df[numeric_cols].values.astype(np.float32)
    
    targets_df = pd.read_csv(r"/mnt/standardized_targets.csv")
    
    # åªä¿ç•™å‰3åˆ—ç›®æ ‡å˜é‡ï¼ˆå°±ä¸šã€CPIã€å·¥ä¸šäº§å‡ºï¼‰
    # è·³è¿‡ç¬¬ä¸€åˆ—DATEï¼Œå–æ¥ä¸‹æ¥çš„3åˆ—
    if len(targets_df.columns) >= 4:
        target_columns = targets_df.columns[1:4].tolist()
    else:
        # å¦‚æœåˆ—æ•°ä¸è¶³ï¼Œå–æ‰€æœ‰æ•°å€¼åˆ—
        target_columns = [col for col in targets_df.columns if col != 'DATE'][:3]
    
    y = targets_df[target_columns].values.astype(np.float32)
    
    print(f"\næ•°æ®å½¢çŠ¶ - è§£é‡Šå˜é‡: {X.shape}, ç›®æ ‡å˜é‡: {y.shape}")
    print(f"ç›®æ ‡åˆ—å: {target_columns}")
    
    # å¤„ç†yä¸­çš„ç¼ºå¤±å€¼
    for i in range(y.shape[1]):
        col = y[:, i]
        col_mean = np.nanmean(col)
        col = np.where(np.isnan(col), col_mean, col)
        y[:, i] = col
    
    # 2. åŠ è½½å¹¶ç®€åŒ–å…ƒæ•°æ®
    metadata_path = r"/mnt/targets_metadata.pkl"
    try:
        meta_data = critical_load_metadata(metadata_path)
    except Exception as e:
        print(f"\nâŒ æ— æ³•åŠ è½½å…ƒæ•°æ®: {str(e)}")
        print("ç¨‹åºç»ˆæ­¢")
        exit(1)
    
    # 3. åˆ†åˆ«å¯¹æ¯ä¸ªç›®æ ‡å˜é‡è¿›è¡Œå¤„ç†
    date_index = pca_df['DATE']
    
    for target_idx in range(y.shape[1]):
        # è·å–ç›®æ ‡å˜é‡åç§°
        target_name = meta_data['target_names'][target_idx]
        
        print(f"\n{'='*60}")
        print(f"â˜… å¤„ç†ç›®æ ‡å˜é‡ [ç´¢å¼•: {target_idx}] - {target_name} ")
        print(f"{'='*60}")
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ - ç‰¹å¾ç»´åº¦ä¿®æ”¹ä¸º8
        model = BayesianEconomicModel(num_features=NUM_FEATURES, hidden_dim=HIDDEN_DIM)
        start_time = time.time()
        print(f"âœ è®­ç»ƒä¸‰å±‚ç½‘ç»œæ¨¡å‹... (ç‰¹å¾æ•° = {NUM_FEATURES}, éšè—å•å…ƒ: {HIDDEN_DIM})")
        model.train(X, y[:, target_idx], num_samples=1000, warmup=300)
        print(f"âœ“ è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time()-start_time:.2f}ç§’")
        
        # 1. è¾“å‡ºå‚æ•°è¯¦ç»†åˆ†å¸ƒ
        describe_parameter_distributions(model.samples)
        plot_parameter_distributions(model.samples)
        
        # 2. åœ¨æ‰€æœ‰æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹
        print("âœ åœ¨å®Œæ•´æ•°æ®é›†ä¸Šé¢„æµ‹...")
        pred_samples = model.predict(X, num_samples=1000)  # å½¢çŠ¶: (1000, n_timesteps)
        
        # è®¡ç®—æ ‡å‡†åŒ–å°ºåº¦ä¸Šçš„RMSE
        mean_pred = pred_samples.mean(0)
        rmse = mean_squared_error(y[:, target_idx], mean_pred, squared=False)
        print(f"âœ§ å…¨æ ·æœ¬RMSE (æ ‡å‡†åŒ–å°ºåº¦): {rmse:.4f}")
        
        # 3. å®Œæ•´é€†å˜æ¢åˆ°åŸå§‹å°ºåº¦
        print("âœ å®Œæ•´é€†å˜æ¢é¢„æµ‹å€¼åˆ°åŸå§‹å°ºåº¦...")
        raw_preds = inverse_transform_target(meta_data, pred_samples, target_idx)
        
        # 4. é€†å˜æ¢çœŸå®å€¼ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        # æ³¨æ„ï¼šyæ˜¯æ ‡å‡†åŒ–çš„å·®åˆ†å€¼ï¼Œéœ€è¦å¤åˆ¶ä¸é¢„æµ‹ç›¸åŒçš„ç»“æ„è¿›è¡Œé€†å˜æ¢
        y_true_expanded = np.tile(y[:, target_idx], (pred_samples.shape[0], 1))
        raw_true = inverse_transform_target(meta_data, y_true_expanded, target_idx)
        raw_true_mean = raw_true.mean(axis=0)  # åŸå§‹çœŸå®å€¼ï¼ˆå‡å€¼ï¼‰
        
        # è®¡ç®—åŸå§‹å°ºåº¦ä¸Šçš„RMSE
        mean_raw_pred = raw_preds.mean(axis=0)
        rmse_raw = mean_squared_error(raw_true_mean, mean_raw_pred, squared=False)
        print(f"âœ§ å…¨æ ·æœ¬RMSE (åŸå§‹å°ºåº¦): {rmse_raw:.4f}")
        
        # 5. ç»˜åˆ¶æ ‡å‡†åŒ–å°ºåº¦é¢„æµ‹åŒºé—´
        print("âœ ç»˜åˆ¶æ ‡å‡†åŒ–å°ºåº¦é¢„æµ‹åŒºé—´...")
        plot_prediction_intervals(
            pred_samples, 
            y[:, target_idx], 
            title=f"æ ‡å‡†åŒ–å°ºåº¦é¢„æµ‹ - {target_name}",
            ylabel="æ ‡å‡†åŒ–å€¼"
        )
        # 6. ç»˜åˆ¶åŸå§‹å°ºåº¦é¢„æµ‹åŒºé—´ï¼ˆå®Œæ•´é€†å˜æ¢åï¼‰
        print("âœ ç»˜åˆ¶åŸå§‹å°ºåº¦é¢„æµ‹åŒºé—´ï¼ˆå«95%ç½®ä¿¡åŒºé—´ï¼‰...")
        raw_preds_reshaped = raw_preds.reshape(1000, -1)  # ç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶
        plot_prediction_intervals(
            raw_preds_reshaped, 
            raw_true_mean, 
            title=f"åŸå§‹å°ºåº¦é¢„æµ‹ - {target_name}",
            ylabel="åŸå§‹å€¼"
        )
        
        # 7. é¢„æµ‹ä¸‹ä¸€ä¸ªæœˆçš„å€¼
        print("âœ é¢„æµ‹ä¸‹ä¸ªæœˆçš„å€¼...")
        last_month_features = X[-1].reshape(1, -1)
        next_month_pred = model.predict(last_month_features, num_samples=1000)  # å½¢çŠ¶: (1000, 1)
        
        # 8. é€†å˜æ¢ä¸‹ä¸ªæœˆçš„é¢„æµ‹å€¼åˆ°åŸå§‹å°ºåº¦
        next_month_raw = inverse_transform_target(meta_data, next_month_pred[:, 0:1], target_idx).flatten()
        # è·å–æœ€åè§‚æµ‹å€¼ä½œä¸ºå‚è€ƒ
        initial_value = meta_data['initial_values'][target_name]
        current_val = raw_true_mean[-1] if len(raw_true_mean) > 0 else 0
        # è®¡ç®—é€†å˜æ¢åçš„ç»Ÿè®¡é‡
        mean_next = next_month_raw.mean() + current_val - initial_value
        std_next = next_month_raw.std()
        lower = np.percentile(next_month_raw, 2.5) + current_val - initial_value
        upper = np.percentile(next_month_raw, 97.5) + current_val - initial_value
        
        print(f"\nâœ§ {target_name} - åŸå§‹åˆå§‹å€¼: {initial_value:.4f}")
        print(f"âœ§ {target_name} - å½“å‰åŸå§‹å€¼: {current_val:.4f}")
        print(f"âœ§ {target_name} - ä¸‹ä¸ªæœˆé¢„æµ‹å€¼ (åŸå§‹å°ºåº¦):")
        print(f"  å‡å€¼é¢„æµ‹: {mean_next:.4f}")
        print(f"  æ ‡å‡†å·®: {std_next:.4f}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{lower:.4f}, {upper:.4f}]")
        if current_val != 0:
            change_percent = 100 * (mean_next - current_val) / current_val
            print(f"  ç›¸æ¯”å½“å‰å€¼å˜åŒ–: {mean_next - current_val:.4f} ({change_percent:.2f}%)")
            print(f"  ç›¸æ¯”åˆå§‹å€¼å˜åŒ–: {mean_next - initial_value:.4f} ({(100 * (mean_next - initial_value)/initial_value):.2f}%)")

                        
        
        # 10. å¯è§†åŒ–å†å²æ•°æ®ä¸é¢„æµ‹
        plt.figure(figsize=(12, 6))
        
        # æœ€å12ä¸ªæœˆçš„å®é™…æ•°æ®
        lookback = min(12, len(date_index))
        
        # ç¡®ä¿åªå±•ç¤ºæœ‰æ•ˆæ•°æ®
        if lookback <= 0:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ç”¨äºç»˜å›¾")
            continue
            
        # æ—¥æœŸç´¢å¼•
        plot_dates = date_index.iloc[-lookback:]
        plot_raw_true = raw_true_mean[-lookback:]
        
        # é¢„æµ‹çš„å†å²å€¼ï¼ˆå–æœ€å12ä¸ªæœˆï¼‰
        plot_pred_mean = mean_raw_pred[-lookback:].flatten()  # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
        pred_lower = np.percentile(raw_preds, 2.5, axis=0)[-lookback:].flatten()  # å±•å¹³ä¸ºä¸€ç»´
        pred_upper = np.percentile(raw_preds, 97.5, axis=0)[-lookback:].flatten()  # å±•å¹³ä¸ºä¸€ç»´
        
        # çœŸå®å€¼
        plt.plot(
            plot_dates, 
            plot_raw_true, 
            'o-', label='å†å²çœŸå®å€¼', color='blue', markersize=6
        )
        
        # é¢„æµ‹å€¼ï¼ˆå†å²éƒ¨åˆ†ï¼‰
        plt.plot(
            plot_dates, 
            plot_pred_mean, 
            's--', label='æ¨¡å‹å›æµ‹', color='red', alpha=0.8, markersize=5
        )
        
        # é¢„æµ‹å€¼ï¼ˆå†å²éƒ¨åˆ†ï¼‰çš„ç½®ä¿¡åŒºé—´
        plt.fill_between(
            plot_dates, 
            pred_lower, 
            pred_upper, 
            color='red', alpha=0.15, label='95%ç½®ä¿¡åŒºé—´'
        )
        
        # ä¸‹ä¸ªæœˆé¢„æµ‹
        next_date = date_index.iloc[-1] + pd.DateOffset(months=1)
        plt.plot(
            next_date, 
            mean_next, 
            'D', markersize=8, 
            label=f'ä¸‹æœˆé¢„æµ‹: {mean_next:.2f}', 
            color='green'
        )
        
        # ç½®ä¿¡åŒºé—´
        plt.errorbar(
            x=[next_date], 
            y=[mean_next], 
            yerr=[[mean_next - lower], [upper - mean_next]], 
            fmt='none', 
            ecolor='black', 
            elinewidth=2.0,
            capsize=8,
            capthick=2,
            label='é¢„æµ‹åŒºé—´'
        )
        
        plt.title(f"{target_name} - ä¸‰å±‚æ¨¡å‹é¢„æµ‹")
        plt.xlabel("æ—¥æœŸ")
        plt.ylabel("åŸå§‹å€¼")
        plt.legend(loc='best', fontsize='small')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{target_name}_model_prediction.png")
        print(f"ğŸ“ˆ é¢„æµ‹å›¾ä¿å­˜ä¸º: {target_name}_model_prediction.png")
        plt.show()
